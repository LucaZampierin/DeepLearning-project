{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Qj3b9-BKHt6M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyJRsinDzJc4",
        "outputId": "f01dfa23-81ca-456e-e91e-7ab7728da90c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PLdbqPt8I0A7"
      },
      "outputs": [],
      "source": [
        "noisy_imgs1, noisy_imgs2 = torch.load('/content/drive/MyDrive/Colab Notebooks/train_data.pkl') # 50000 x 3 x 32 x 32\n",
        "noisy_val, clean_val = torch.load('/content/drive/MyDrive/Colab Notebooks/val_data.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "m7fkiwvXJmom"
      },
      "outputs": [],
      "source": [
        "def psnr(denoised, ground_truth):\n",
        "  mse = torch.mean((denoised - ground_truth) ** 2)\n",
        "  return -10*torch.log10(mse + 10 ** -8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, noise_img, ground_truth):\n",
        "  psnr_tot = 0\n",
        "  if torch.cuda.is_available():\n",
        "      noise_img, ground_truth = noise_img.to(\"cuda\"), ground_truth.to(\"cuda\")\n",
        "  for i in range(noise_img.size(0)):\n",
        "    denoised = model(noise_img[i].view(1, 3, 32, 32))\n",
        "    print(noise_img[i])\n",
        "    psnr_val = psnr(denoised, ground_truth[i]).item()\n",
        "    print(psnr_val)\n",
        "    psnr_tot += psnr_val\n",
        "  psnr_tot /= noise_img.size(0)\n",
        "  return psnr_tot"
      ],
      "metadata": {
        "id": "x7krHeH9NWOl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7t9yy00H7PHb"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_input, train_target, criterion, optimizer, mini_batch_size=4, epochs=500, normalize=False):\n",
        "    if torch.cuda.is_available():\n",
        "      model.to(\"cuda\")\n",
        "      train_input, train_target = train_input.to(\"cuda\"), train_target.to(\"cuda\")\n",
        "    if normalize:\n",
        "      mu, std = train_input.mean(), train_input.std()\n",
        "      train_input.sub_(mu).div_(std)\n",
        "\n",
        "    for e in range(epochs):\n",
        "      print(e)\n",
        "      #TODO ADD PRINT OF CURRENT LOSS TO MAKE SURE THERE IS NO NAN\n",
        "      for b in range(0, train_input.size(0), mini_batch_size):\n",
        "          output = model(train_input.narrow(0, b, mini_batch_size))\n",
        "          loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YDnQT6D-K3SM"
      },
      "outputs": [],
      "source": [
        "class SuperbModel(torch.nn.Module):\n",
        "    def __init__(self, transposed_conv=False):\n",
        "        super().__init__()\n",
        "        self.enc_conv0 = nn.Conv2d(3, 48, (3, 3), padding='same')\n",
        "        self.enc_conv1 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv2 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv3 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv4 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv5 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv6 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.upsample5 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenation \n",
        "        self.dec_conv5a = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.dec_conv5b = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.upsample4 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenation\n",
        "        self.dec_conv4a = nn.Conv2d(144, 96, (3, 3), padding='same')\n",
        "        self.dec_conv4b = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.upsample3 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenation\n",
        "        self.dec_conv3a = nn.Conv2d(144, 96, (3, 3), padding='same')\n",
        "        self.dec_conv3b = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenatio\n",
        "        self.dec_conv2a = nn.Conv2d(144, 96, (3, 3), padding='same')\n",
        "        self.dec_conv2b = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenation\n",
        "        self.custom = nn.Conv2d(144, 99, (3, 3), padding='same')\n",
        "        self.dec_conv1a = nn.Conv2d(96 + 3, 64, (3, 3), padding='same')\n",
        "        self.dec_conv1b = nn.Conv2d(64, 32, (3, 3), padding='same')\n",
        "        self.dec_conv1c = nn.Conv2d(32, 3, (3, 3), padding='same')\n",
        "                  #what does linear activation mean (output unchanged?)\n",
        "\n",
        "    def forward(self, x):\n",
        "      input = x.clone()\n",
        "      x = F.leaky_relu(self.enc_conv0(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.enc_conv1(x), negative_slope=0.1)\n",
        "      #pool1 = F.max_pool2d(x, 2)\n",
        "      #x = F.leaky_relu(self.enc_conv2(pool1), negative_slope=0.1)\n",
        "      '''pool2 = F.max_pool2d(x, 2)\n",
        "      x = F.leaky_relu(self.enc_conv3(pool2), negative_slope=0.1)'''\n",
        "      pool3 = F.max_pool2d(x, 2) #MAKE SURE THEY HAVE CORRECT VALUES AND THEY ARE NOT CHANGED BY FOLLOWING OPERATIONS\n",
        "      x = F.leaky_relu(self.enc_conv4(pool3), negative_slope=0.1)\n",
        "      pool4 = F.max_pool2d(x, 2)\n",
        "      x = F.leaky_relu(self.enc_conv5(pool4), negative_slope=0.1)\n",
        "      ''' pool5 = F.max_pool2d(x, 2)\n",
        "      x = F.leaky_relu(self.enc_conv6(pool5), negative_slope=0.1)\n",
        "      x = self.upsample5(x)'''\n",
        "      x = torch.cat((x, pool4), dim=1)#.view(-1, 96, 2, 2) #MAKE SURE THEY ARE STACKED ON THE CORRECT DIMENSIONS\n",
        "      '''x = F.leaky_relu(self.dec_conv5a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv5b(x), negative_slope=0.1)\n",
        "      x = self.upsample4(x) \n",
        "      x = torch.cat((x, pool3), dim=1)\n",
        "      \n",
        "      x = F.leaky_relu(self.dec_conv4a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv4b(x), negative_slope=0.1)\n",
        "      x = self.upsample3(x) \n",
        "      x = torch.cat((x, pool2), dim=1)'''\n",
        "      x = self.upsample3(x) # custom\n",
        "      x = torch.cat((x, pool3), dim=1) #custom\n",
        "      x = F.leaky_relu(self.dec_conv3a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv3b(x), negative_slope=0.1)\n",
        "      x = self.upsample2(x) \n",
        "      #x = torch.cat((x, pool1), dim=1)\n",
        "      '''\n",
        "      x = F.leaky_relu(self.dec_conv2a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv2b(x), negative_slope=0.1)\n",
        "      x = self.upsample1(x) '''\n",
        "      x = torch.cat((x, input), dim=1)\n",
        "      #x = F.leaky_relu(self.custom(x), negative_slope=0.1) #custom\n",
        "      x = F.leaky_relu(self.dec_conv1a(x), negative_slope=0.1)\n",
        "      x = self.upsample1(x) #custom\n",
        "      x = F.leaky_relu(self.dec_conv1b(x), negative_slope=0.1)\n",
        "      x = F.max_pool2d(x, 2) #custom\n",
        "      x = self.dec_conv1c(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QD5d-nJvzFm7"
      },
      "outputs": [],
      "source": [
        "class LeanerModel(torch.nn.Module):\n",
        "    def __init__(self, transposed_conv=False):\n",
        "        super().__init__()\n",
        "        self.enc_conv0 = nn.Conv2d(3, 48, (3, 3), padding='same')\n",
        "        self.enc_conv1 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "        self.enc_conv3 = nn.Conv2d(48, 48, (3, 3), padding='same')\n",
        "                  # concatenation\n",
        "        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenatio\n",
        "        self.dec_conv2a = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.dec_conv2b = nn.Conv2d(96, 96, (3, 3), padding='same')\n",
        "        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "                  # concatenation\n",
        "        self.custom = nn.Conv2d(144, 99, (3, 3), padding='same')\n",
        "        self.dec_conv1a = nn.Conv2d(96 + 3, 64, (3, 3), padding='same')\n",
        "        self.dec_conv1b = nn.Conv2d(64, 32, (3, 3), padding='same')\n",
        "        self.dec_conv1c = nn.Conv2d(32, 3, (3, 3), padding='same')\n",
        "                  #what does linear activation mean (output unchanged?)\n",
        "\n",
        "    def forward(self, x):\n",
        "      input = x.clone()\n",
        "      x = F.leaky_relu(self.enc_conv0(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.enc_conv1(x), negative_slope=0.1)\n",
        "      pool1 = F.max_pool2d(x, 2) #48x16x16\n",
        "      x = F.leaky_relu(self.enc_conv3(pool1), negative_slope=0.1)\n",
        "      #pool3 = F.max_pool2d(x, 2) #48x8x8\n",
        "      #x = self.upsample2(x) #48x32x32\n",
        "      x = torch.cat((x, pool1), dim=1)#96x16x16\n",
        "      x = F.leaky_relu(self.dec_conv2a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv2b(x), negative_slope=0.1)\n",
        "      x = self.upsample1(x)  #96x32x32\n",
        "      x = torch.cat((x, input), dim=1) #99x32x32\n",
        "      x = F.leaky_relu(self.dec_conv1a(x), negative_slope=0.1)\n",
        "      x = F.leaky_relu(self.dec_conv1b(x), negative_slope=0.1)\n",
        "      x = self.dec_conv1c(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "S2e3MRORcy05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9473f507-1b93-4a16-c8ea-126560fa7e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ogXoYG1kJL19",
        "outputId": "dd1cc70e-314e-42a0-9ac7-1117b2178760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-670e4e085980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeanerModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy_imgs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy_imgs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mymodule.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-949c6bbffe8d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, criterion, optimizer, mini_batch_size, epochs, normalize)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = LeanerModel()\n",
        "train_model(model, noisy_imgs1.float(), noisy_imgs2.float(), nn.MSELoss(), torch.optim.Adam(model.parameters()), 500, 100)\n",
        "torch.save(model.state_dict(), 'mymodule.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeanerModel()\n",
        "model.load_state_dict(torch.load('mymodule.pt'))\n",
        "print(model.state_dict())\n",
        "#print(validate(model, noisy_val.float(), clean_val.float()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un1oCewLJC9o",
        "outputId": "cf935d8f-4ce4-4857-b044-f1c357a65d49"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('enc_conv0.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('enc_conv0.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('enc_conv1.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('enc_conv1.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('enc_conv3.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('enc_conv3.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('dec_conv2a.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('dec_conv2a.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])), ('dec_conv2b.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('dec_conv2b.bias', tensor([inf, inf, inf, nan, inf, inf, inf, nan, nan, nan, nan, inf, nan, inf, nan, inf, inf, inf, nan, nan, inf, nan, inf, inf,\n",
            "        inf, inf, nan, nan, nan, nan, nan, inf, nan, nan, nan, nan, nan, inf, nan, nan, inf, nan, nan, inf, nan, nan, nan, nan,\n",
            "        inf, inf, nan, inf, nan, inf, inf, inf, inf, inf, inf, inf, nan, inf, nan, nan, inf, nan, inf, nan, inf, inf, nan, nan,\n",
            "        inf, nan, nan, inf, nan, nan, inf, nan, inf, nan, nan, nan, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, inf])), ('custom.weight', tensor([[[[ 0.0176,  0.0039,  0.0275],\n",
            "          [ 0.0138,  0.0273,  0.0040],\n",
            "          [-0.0187, -0.0075, -0.0180]],\n",
            "\n",
            "         [[ 0.0221,  0.0176, -0.0140],\n",
            "          [-0.0001,  0.0014, -0.0068],\n",
            "          [-0.0015, -0.0242,  0.0172]],\n",
            "\n",
            "         [[ 0.0143,  0.0078,  0.0167],\n",
            "          [-0.0256,  0.0269, -0.0246],\n",
            "          [ 0.0205,  0.0174, -0.0164]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0008, -0.0254,  0.0159],\n",
            "          [-0.0151,  0.0274,  0.0173],\n",
            "          [-0.0083, -0.0183, -0.0132]],\n",
            "\n",
            "         [[-0.0135,  0.0273,  0.0018],\n",
            "          [ 0.0113, -0.0179,  0.0105],\n",
            "          [ 0.0062,  0.0066,  0.0046]],\n",
            "\n",
            "         [[ 0.0007, -0.0253,  0.0114],\n",
            "          [ 0.0165,  0.0164, -0.0112],\n",
            "          [-0.0186,  0.0109, -0.0268]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0218,  0.0187,  0.0233],\n",
            "          [-0.0230,  0.0038, -0.0222],\n",
            "          [ 0.0136, -0.0087, -0.0002]],\n",
            "\n",
            "         [[-0.0171,  0.0039, -0.0226],\n",
            "          [ 0.0034,  0.0207,  0.0027],\n",
            "          [ 0.0005,  0.0068, -0.0192]],\n",
            "\n",
            "         [[-0.0119, -0.0090,  0.0059],\n",
            "          [ 0.0142,  0.0237, -0.0146],\n",
            "          [-0.0211,  0.0140, -0.0037]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0189, -0.0096, -0.0191],\n",
            "          [ 0.0190, -0.0233,  0.0205],\n",
            "          [ 0.0224,  0.0014,  0.0187]],\n",
            "\n",
            "         [[-0.0015,  0.0131,  0.0175],\n",
            "          [ 0.0262, -0.0116, -0.0212],\n",
            "          [ 0.0044, -0.0109, -0.0122]],\n",
            "\n",
            "         [[ 0.0128,  0.0142,  0.0120],\n",
            "          [-0.0144, -0.0122, -0.0186],\n",
            "          [-0.0172, -0.0047, -0.0187]]],\n",
            "\n",
            "\n",
            "        [[[-0.0147,  0.0087, -0.0241],\n",
            "          [-0.0265,  0.0275, -0.0210],\n",
            "          [ 0.0005,  0.0070,  0.0107]],\n",
            "\n",
            "         [[-0.0205, -0.0111, -0.0165],\n",
            "          [-0.0221,  0.0270, -0.0042],\n",
            "          [ 0.0067,  0.0040, -0.0113]],\n",
            "\n",
            "         [[-0.0272, -0.0255,  0.0174],\n",
            "          [ 0.0229,  0.0006,  0.0015],\n",
            "          [ 0.0271, -0.0170,  0.0201]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0118, -0.0111,  0.0191],\n",
            "          [ 0.0090,  0.0127, -0.0200],\n",
            "          [ 0.0232,  0.0095,  0.0147]],\n",
            "\n",
            "         [[-0.0170,  0.0040, -0.0264],\n",
            "          [ 0.0277,  0.0005,  0.0008],\n",
            "          [ 0.0070,  0.0111, -0.0168]],\n",
            "\n",
            "         [[ 0.0126, -0.0022,  0.0049],\n",
            "          [ 0.0185,  0.0255,  0.0174],\n",
            "          [ 0.0125,  0.0250,  0.0086]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0272, -0.0085, -0.0104],\n",
            "          [ 0.0201, -0.0116,  0.0164],\n",
            "          [ 0.0055, -0.0063, -0.0102]],\n",
            "\n",
            "         [[ 0.0025,  0.0256, -0.0209],\n",
            "          [-0.0017,  0.0129, -0.0130],\n",
            "          [-0.0238,  0.0075,  0.0188]],\n",
            "\n",
            "         [[ 0.0275,  0.0184, -0.0012],\n",
            "          [-0.0245, -0.0015, -0.0224],\n",
            "          [ 0.0150, -0.0267,  0.0104]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0053,  0.0154, -0.0224],\n",
            "          [-0.0199, -0.0164,  0.0207],\n",
            "          [ 0.0038, -0.0164,  0.0030]],\n",
            "\n",
            "         [[ 0.0098, -0.0019,  0.0157],\n",
            "          [ 0.0196,  0.0100,  0.0030],\n",
            "          [ 0.0033,  0.0090,  0.0047]],\n",
            "\n",
            "         [[ 0.0224, -0.0073, -0.0198],\n",
            "          [-0.0277,  0.0092,  0.0078],\n",
            "          [-0.0170,  0.0116, -0.0028]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0076, -0.0009, -0.0025],\n",
            "          [ 0.0018,  0.0063,  0.0034],\n",
            "          [-0.0027, -0.0206, -0.0145]],\n",
            "\n",
            "         [[ 0.0029,  0.0218, -0.0009],\n",
            "          [ 0.0207,  0.0157,  0.0177],\n",
            "          [-0.0167, -0.0247, -0.0040]],\n",
            "\n",
            "         [[-0.0206,  0.0002,  0.0229],\n",
            "          [ 0.0064, -0.0187, -0.0002],\n",
            "          [-0.0140, -0.0156,  0.0026]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0254, -0.0166, -0.0201],\n",
            "          [-0.0198,  0.0043,  0.0047],\n",
            "          [-0.0191,  0.0263,  0.0261]],\n",
            "\n",
            "         [[-0.0013, -0.0117,  0.0235],\n",
            "          [ 0.0105,  0.0094,  0.0269],\n",
            "          [ 0.0254, -0.0188,  0.0061]],\n",
            "\n",
            "         [[-0.0027,  0.0147,  0.0133],\n",
            "          [-0.0012, -0.0228, -0.0186],\n",
            "          [ 0.0110, -0.0236, -0.0115]]],\n",
            "\n",
            "\n",
            "        [[[-0.0198, -0.0235,  0.0087],\n",
            "          [-0.0214, -0.0025,  0.0018],\n",
            "          [ 0.0230,  0.0262,  0.0090]],\n",
            "\n",
            "         [[ 0.0191,  0.0201,  0.0183],\n",
            "          [ 0.0115, -0.0073,  0.0244],\n",
            "          [ 0.0257,  0.0078,  0.0242]],\n",
            "\n",
            "         [[-0.0118,  0.0004, -0.0088],\n",
            "          [-0.0150, -0.0127,  0.0150],\n",
            "          [ 0.0253, -0.0071,  0.0196]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0013, -0.0143, -0.0168],\n",
            "          [ 0.0252, -0.0095, -0.0177],\n",
            "          [ 0.0214,  0.0194, -0.0081]],\n",
            "\n",
            "         [[-0.0228, -0.0128, -0.0129],\n",
            "          [ 0.0027, -0.0113,  0.0046],\n",
            "          [-0.0089,  0.0251,  0.0119]],\n",
            "\n",
            "         [[-0.0015, -0.0126, -0.0011],\n",
            "          [ 0.0149, -0.0057,  0.0051],\n",
            "          [-0.0022, -0.0212,  0.0117]]]])), ('custom.bias', tensor([-0.0122,  0.0032,  0.0084, -0.0091,  0.0227,  0.0277,  0.0242,  0.0194,\n",
            "        -0.0089, -0.0023, -0.0200, -0.0020, -0.0159,  0.0228, -0.0156,  0.0230,\n",
            "         0.0189, -0.0255,  0.0008, -0.0140, -0.0247, -0.0232,  0.0129,  0.0116,\n",
            "        -0.0006, -0.0273, -0.0105, -0.0059, -0.0190, -0.0057,  0.0028, -0.0004,\n",
            "         0.0161, -0.0160, -0.0202, -0.0079,  0.0275, -0.0258, -0.0101, -0.0039,\n",
            "         0.0232,  0.0259,  0.0036,  0.0040, -0.0171, -0.0110,  0.0170, -0.0062,\n",
            "         0.0266,  0.0030,  0.0211,  0.0259, -0.0267, -0.0231, -0.0047,  0.0216,\n",
            "         0.0005,  0.0180,  0.0139, -0.0074,  0.0125, -0.0266, -0.0060, -0.0125,\n",
            "        -0.0244,  0.0174,  0.0162,  0.0049, -0.0065, -0.0087, -0.0116, -0.0006,\n",
            "        -0.0169,  0.0116,  0.0086, -0.0255, -0.0222, -0.0241, -0.0153, -0.0138,\n",
            "         0.0028, -0.0202,  0.0060,  0.0063,  0.0128,  0.0010, -0.0224, -0.0250,\n",
            "         0.0274, -0.0050,  0.0018, -0.0146, -0.0022,  0.0048,  0.0237, -0.0233,\n",
            "         0.0070, -0.0142, -0.0046])), ('dec_conv1a.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('dec_conv1a.bias', tensor([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
            "        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,\n",
            "        inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf])), ('dec_conv1b.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('dec_conv1b.bias', tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan])), ('dec_conv1c.weight', tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]])), ('dec_conv1c.bias', tensor([nan, nan, nan]))])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DummyCodeDL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}